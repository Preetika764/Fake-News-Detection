{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"politifact_variational_autoencoder.ipynb","provenance":[{"file_id":"1Wjy3xp1ixYFJjgQN5jr3kbmgKbV6Nm04","timestamp":1638159688896}],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9dhf6ZjA8Izr","executionInfo":{"status":"ok","timestamp":1645593411220,"user_tz":-330,"elapsed":25915,"user":{"displayName":"SARANSH GOEL","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"10247441952356680073"}},"outputId":"6b7b07b6-b3ab-49c0-96cd-5958102c24bb"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","metadata":{"id":"kEoqWd3wrI-m","executionInfo":{"status":"ok","timestamp":1645593411222,"user_tz":-330,"elapsed":24,"user":{"displayName":"SARANSH GOEL","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"10247441952356680073"}}},"source":["class Vocabulary(object):\n","    \"\"\"Class to process text and extract vocabulary for mapping\"\"\"\n","\n","    def __init__(self, token_to_idx=None):\n","        \"\"\"\n","        Args:\n","            token_to_idx (dict): a pre-existing map of tokens to indices\n","        \"\"\"\n","\n","        if token_to_idx is None:\n","            token_to_idx = {}\n","        self._token_to_idx = token_to_idx\n","\n","        self._idx_to_token = {idx: token \n","                              for token, idx in self._token_to_idx.items()}\n","\n","    def to_serializable(self):\n","        \"\"\" returns a dictionary that can be serialized \"\"\"\n","        return {'token_to_idx': self._token_to_idx}\n","\n","    @classmethod\n","    def from_serializable(cls, contents):\n","        \"\"\" instantiates the Vocabulary from a serialized dictionary \"\"\"\n","        return cls(**contents)\n","\n","    def add_token(self, token):\n","        \"\"\"Update mapping dicts based on the token.\n","        Args:\n","            token (str): the item to add into the Vocabulary\n","        Returns:\n","            index (int): the integer corresponding to the token\n","        \"\"\"\n","        if token in self._token_to_idx:\n","            index = self._token_to_idx[token]\n","        else:\n","            index = len(self._token_to_idx)\n","            self._token_to_idx[token] = index\n","            self._idx_to_token[index] = token\n","        return index\n","\n","    def lookup_token(self, token):\n","        \"\"\"Retrieve the index associated with the token \n","        \n","        Args:\n","            token (str): the token to look up \n","        Returns:\n","            index (int): the index corresponding to the token\n","        \"\"\"\n","        return self._token_to_idx[token]\n","\n","    def lookup_index(self, index):\n","        \"\"\"Return the token associated with the index\n","        \n","        Args: \n","            index (int): the index to look up\n","        Returns:\n","            token (str): the token corresponding to the index\n","        Raises:\n","            KeyError: if the index is not in the Vocabulary\n","        \"\"\"\n","        if index not in self._idx_to_token:\n","            raise KeyError(\"the index (%d) is not in the Vocabulary\" % index)\n","        return self._idx_to_token[index]\n","\n","    def __str__(self):\n","        return \"<Vocabulary(size=%d)>\" % len(self)\n","\n","    def __len__(self):\n","        return len(self._token_to_idx)\n","    \n","    \n","class SequenceVocabulary(Vocabulary):\n","    def __init__(self, token_to_idx=None, unk_token=\"<UNK>\",\n","                 mask_token=\"<MASK>\", begin_seq_token=\"<BEGIN>\",\n","                 end_seq_token=\"<END>\"):\n","\n","        super(SequenceVocabulary, self).__init__(token_to_idx)\n","\n","        self._mask_token = mask_token\n","        self._unk_token = unk_token\n","        self._begin_seq_token = begin_seq_token\n","        self._end_seq_token = end_seq_token\n","\n","        self.mask_index = self.add_token(self._mask_token)\n","        self.unk_index = self.add_token(self._unk_token)\n","        self.begin_seq_index = self.add_token(self._begin_seq_token)\n","        self.end_seq_index = self.add_token(self._end_seq_token)\n","\n","    def to_serializable(self):\n","        contents = super(SequenceVocabulary, self).to_serializable()\n","        contents.update({'unk_token': self._unk_token,\n","                         'mask_token': self._mask_token,\n","                         'begin_seq_token': self._begin_seq_token,\n","                         'end_seq_token': self._end_seq_token})\n","        return contents\n","\n","    def lookup_token(self, token):\n","        \"\"\"Retrieve the index associated with the token \n","          or the UNK index if token isn't present.\n","        \n","        Args:\n","            token (str): the token to look up \n","        Returns:\n","            index (int): the index corresponding to the token\n","        Notes:\n","            `unk_index` needs to be >=0 (having been added into the Vocabulary) \n","              for the UNK functionality \n","        \"\"\"\n","        if self.unk_index >= 0:\n","            return self._token_to_idx.get(token, self.unk_index)\n","        else:\n","            return self._token_to_idx[token]   "],"execution_count":2,"outputs":[]},{"cell_type":"code","metadata":{"id":"OMxZNE58W41M","executionInfo":{"status":"ok","timestamp":1645593411223,"user_tz":-330,"elapsed":19,"user":{"displayName":"SARANSH GOEL","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"10247441952356680073"}}},"source":["import numpy as np\n","from collections import Counter\n","import string\n","\n","class NewsVectorizer(object):\n","    \"\"\" The Vectorizer which coordinates the Vocabularies and puts them to use\"\"\"    \n","    def __init__(self, text_vocab, type_vocab):\n","        self.text_vocab = text_vocab\n","        self.type_vocab = type_vocab\n","\n","    def vectorize(self,text, vector_length=-1):\n","        \"\"\"\n","        Args:\n","            title (str): the string of words separated by a space\n","            vector_length (int): an argument for forcing the length of index vector\n","        Returns:\n","            the vetorized title (numpy.array)\n","        \"\"\"\n","        text = str(text)\n","        indices = [self.text_vocab.begin_seq_index]\n","        indices.extend(self.text_vocab.lookup_token(token) \n","                       for token in text.split(\" \"))\n","        indices.append(self.text_vocab.end_seq_index)\n","\n","        if vector_length < 0:\n","            vector_length = len(indices)\n","\n","        out_vector = np.zeros(vector_length, dtype=np.int64)\n","        out_vector[:len(indices)] = indices\n","        out_vector[len(indices):] = self.text_vocab.mask_index\n","\n","        return out_vector\n","\n","    @classmethod\n","    def from_dataframe(cls, news_df, cutoff=25):\n","        \"\"\"Instantiate the vectorizer from the dataset dataframe\n","        \n","        Args:\n","            news_df (pandas.DataFrame): the target dataset\n","            cutoff (int): frequency threshold for including in Vocabulary \n","        Returns:\n","            an instance of the NewsVectorizer\n","        \"\"\"\n","        type_vocab = Vocabulary()        \n","        for type in sorted(set(news_df.type)):\n","            type_vocab.add_token(type)\n","\n","        word_counts = Counter()\n","        for text in news_df.text:\n","            text = str(text)\n","            for token in text.split(' '):\n","                if token not in string.punctuation:\n","                    word_counts[token] += 1\n","        \n","        text_vocab = SequenceVocabulary()\n","        for word, word_count in word_counts.items():\n","            if word_count >= cutoff:\n","                text_vocab.add_token(word)\n","        \n","        return cls(text_vocab, type_vocab)\n","\n","    def to_serializable(self):\n","        return {'text_vocab': self.text_vocab.to_serializable(),\n","                'type_vocab': self.type_vocab.to_serializable()}"],"execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"id":"Z0QxkaQwrH9P","executionInfo":{"status":"ok","timestamp":1645593416964,"user_tz":-330,"elapsed":5755,"user":{"displayName":"SARANSH GOEL","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"10247441952356680073"}}},"source":["import pandas as pd\n","import torch\n","from torch.utils.data import Dataset, DataLoader\n","import json\n","\n","class NewsDataset(Dataset):\n","    def __init__(self, news_df, vectorizer):\n","        \"\"\"\n","        Args:\n","            news_df (pandas.DataFrame): the dataset\n","            vectorizer (NewsVectorizer): vectorizer instantiated from dataset\n","        \"\"\"\n","        self.news_df = news_df\n","        self._vectorizer = vectorizer\n","\n","        # +1 if only using begin_seq, +2 if using both begin and end seq tokens\n","        measure_len = lambda context: len(str(context).split(\" \"))\n","        self._max_seq_length = max(map(measure_len, news_df.text)) + 2\n","        \n","\n","        self.train_df = self.news_df[self.news_df.split=='train']\n","        self.train_size = len(self.train_df)\n","\n","        self.val_df = self.news_df[self.news_df.split=='val']\n","        self.validation_size = len(self.val_df)\n","\n","        self.test_df = self.news_df[self.news_df.split=='test']\n","        self.test_size = len(self.test_df)\n","\n","        self._lookup_dict = {'train': (self.train_df, self.train_size),\n","                             'val': (self.val_df, self.validation_size),\n","                             'test': (self.test_df, self.test_size)}\n","\n","        self.set_split('train')\n","        \n","        \n","    @classmethod\n","    def load_dataset_and_make_vectorizer(cls, news_csv):\n","        \"\"\"Load dataset and make a new vectorizer from scratch\n","        \n","        Args:\n","            surname_csv (str): location of the dataset\n","        Returns:\n","            an instance of SurnameDataset\n","        \"\"\"\n","        news_df = pd.read_csv(news_csv)\n","        train_news_df = news_df[news_df.split=='train']\n","        return cls(news_df, NewsVectorizer.from_dataframe(train_news_df))\n","\n","    @classmethod\n","    def load_dataset_and_load_vectorizer(cls, news_csv, vectorizer_filepath):\n","        \"\"\"Load dataset and the corresponding vectorizer. \n","        Used in the case in the vectorizer has been cached for re-use\n","        \n","        Args:\n","            surname_csv (str): location of the dataset\n","            vectorizer_filepath (str): location of the saved vectorizer\n","        Returns:\n","            an instance of SurnameDataset\n","        \"\"\"\n","        news_df = pd.read_csv(news_csv)\n","        vectorizer = cls.load_vectorizer_only(vectorizer_filepath)\n","        return cls(news_df, vectorizer)\n","\n","    @staticmethod\n","    def load_vectorizer_only(vectorizer_filepath):\n","        \"\"\"a static method for loading the vectorizer from file\n","        \n","        Args:\n","            vectorizer_filepath (str): the location of the serialized vectorizer\n","        Returns:\n","            an instance of SurnameVectorizer\n","        \"\"\"\n","        with open(vectorizer_filepath) as fp:\n","            return NewsVectorizer.from_serializable(json.load(fp))\n","\n","    def save_vectorizer(self, vectorizer_filepath):\n","        \"\"\"saves the vectorizer to disk using json\n","        \n","        Args:\n","            vectorizer_filepath (str): the location to save the vectorizer\n","        \"\"\"\n","        with open(vectorizer_filepath, \"w\") as fp:\n","            json.dump(self._vectorizer.to_serializable(), fp)\n","\n","    def get_vectorizer(self):\n","        \"\"\" returns the vectorizer \"\"\"\n","        return self._vectorizer\n","\n","    def set_split(self, split=\"train\"):\n","        \"\"\" selects the splits in the dataset using a column in the dataframe \"\"\"\n","        self._target_split = split\n","        self._target_df, self._target_size = self._lookup_dict[split]\n","\n","    def __len__(self):\n","        return self._target_size\n","\n","    def __getitem__(self, index):\n","        \"\"\"the primary entry point method for PyTorch datasets\n","        \n","        Args:\n","            index (int): the index to the data point \n","        Returns:\n","            a dictionary holding the data point's features (x_data) and label (y_target)\n","        \"\"\"\n","        row = self._target_df.iloc[index]\n","\n","        text_vector = \\\n","            self._vectorizer.vectorize(row.text, self._max_seq_length)\n","\n","        type_index = \\\n","            self._vectorizer.type_vocab.lookup_token(row.type)\n","\n","        return {'x_data': text_vector,\n","                'y_target': type_index}\n","\n","    def get_num_batches(self, batch_size):\n","        \"\"\"Given a batch size, return the number of batches in the dataset\n","        \n","        Args:\n","            batch_size (int)\n","        Returns:\n","            number of batches in the dataset\n","        \"\"\"\n","        return len(self) // batch_size\n","\n","def generate_batches(dataset, batch_size, shuffle=True, drop_last=True, device=\"cpu\"): \n","    \"\"\"\n","        A generator function which wraps the PyTorch DataLoader. It will \n","        ensure each tensor is on the write device location.\n","    \"\"\"\n","    dataloader = DataLoader(dataset=dataset, batch_size=batch_size,shuffle=shuffle, drop_last=drop_last)\n","\n","    for data_dict in dataloader:\n","        out_data_dict = {}\n","        for name, tensor in data_dict.items():\n","            out_data_dict[name] = data_dict[name].to(device)\n","        yield out_data_dict"],"execution_count":4,"outputs":[]},{"cell_type":"code","metadata":{"id":"-QisOwmJMOeO","executionInfo":{"status":"ok","timestamp":1645593416969,"user_tz":-330,"elapsed":46,"user":{"displayName":"SARANSH GOEL","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"10247441952356680073"}}},"source":["import numpy as np\n","import torch\n","\n","def load_glove_from_file(glove_filepath):\n","    \"\"\"\n","    Load the GloVe embeddings \n","    \n","    Args:\n","        glove_filepath (str): path to the glove embeddings file \n","    Returns:\n","        word_to_index (dict), embeddings (numpy.ndarary)\n","    \"\"\"\n","\n","    word_to_index = {}\n","    embeddings = []\n","    with open(glove_filepath, \"r\") as fp:\n","        for index, line in enumerate(fp):\n","            line = line.split(\" \") # each line: word num1 num2 ...\n","            word_to_index[line[0]] = index # word = line[0] \n","            embedding_i = np.array([float(val) for val in line[1:]])\n","            embeddings.append(embedding_i)\n","    return word_to_index, np.stack(embeddings)\n","\n","def make_embedding_matrix(glove_filepath, words):\n","    \"\"\"\n","    Create embedding matrix for a specific set of words.\n","    \n","    Args:\n","        glove_filepath (str): file path to the glove embeddigns\n","        words (list): list of words in the dataset\n","    \"\"\"\n","    word_to_idx, glove_embeddings = load_glove_from_file(glove_filepath)\n","    embedding_size = glove_embeddings.shape[1]\n","    \n","    final_embeddings = np.zeros((len(words), embedding_size))\n","\n","    for i, word in enumerate(words):\n","        if word in word_to_idx:\n","            final_embeddings[i, :] = glove_embeddings[word_to_idx[word]]\n","        else:\n","            embedding_i = torch.ones(1, embedding_size)\n","            torch.nn.init.xavier_uniform_(embedding_i)\n","            final_embeddings[i, :] = embedding_i\n","            \n","    return final_embeddings"],"execution_count":5,"outputs":[]},{"cell_type":"code","metadata":{"id":"kMgHywSrMi3l","executionInfo":{"status":"ok","timestamp":1645593416973,"user_tz":-330,"elapsed":41,"user":{"displayName":"SARANSH GOEL","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"10247441952356680073"}}},"source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torch.nn.functional as F\n","\n","import timeit\n","import numpy as np\n","import random\n","import datetime\n","\n","def repackage_hidden(h):\n","    \"\"\"Wraps hidden states in new Tensors, to detach them from their history.\"\"\"\n","\n","    if isinstance(h, torch.Tensor):\n","        return h.detach()\n","    else:\n","        return tuple(repackage_hidden(v) for v in h)\n","\n","class TextRnnVAE(nn.Module):\n","    \n","    def __init__(self, args, pretrained_embeddings):\n","        super(TextRnnVAE, self).__init__()\n","\n","        self.args = args\n","        pretrained_embeddings = torch.from_numpy(pretrained_embeddings).float()\n","        self.emb = nn.Embedding(embedding_dim=args.embedding_size,\n","                                num_embeddings=args.num_embeddings,\n","                                padding_idx=args.padding_idx,\n","                                _weight=pretrained_embeddings)\n","        \n","        self.encoder = Encoder(args, self.emb)\n","        self.decoder = Decoder(args, self.emb)\n","        self.encoder.to(args.device)\n","        self.decoder.to(args.device)\n","        self.encoder_optimizer = optim.Adam(self.encoder.parameters(), lr=0.001)\n","        self.decoder_optimizer = optim.Adam(self.decoder.parameters(), lr=0.001)\n","        encoder_scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer=self.encoder_optimizer,\n","                                           mode='min', factor=0.5,\n","                                           patience=1)\n","        decoder_scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer=self.decoder_optimizer,\n","                                           mode='min', factor=0.5,\n","                                           patience=1)\n","       \n","    def train(self):\n","        self.encoder.train()\n","        self.decoder.train()\n","\n","    def train_epoch(self, epoch, batch_generator):\n","        epoch_loss = 0.0\n","        \n","        for batch_index, batch_dict in enumerate(batch_generator):\n","            # Get size of batch (can differ between batches due to bucketing)\n","            inputs = batch_dict['x_data']\n","            batch_size = inputs.shape[0]\n","\n","            # Convert to tensors and move to device\n","            inputs = torch.tensor(inputs).to(self.args.device)\n","\n","            # Train batch and get batch loss\n","            batch_loss = self.train_batch(inputs)\n","        \n","            # Update epoch loss given als batch loss\n","            epoch_loss += batch_loss\n","\n","            print('Epoch: {} #batches {}, loss: {:.8f}'.format(epoch + 1, batch_index + 1, (batch_loss / ((batch_index + 1) * batch_size))))\n","\n","        print()\n","        return epoch_loss\n","\n","    def train_batch(self, inputs):\n","        batch_size, num_steps = inputs.shape\n","\n","        # Initialize hidden state\n","        self.encoder.hidden = self.encoder.init_hidden(batch_size)\n","        \n","        # Zero gradients of both optimizers\n","        self.encoder_optimizer.zero_grad()\n","        self.decoder_optimizer.zero_grad()\n","\n","        mean, logv, z = self.encoder(inputs)\n","\n","        loss,_ = self.decoder(inputs, z)\n","\n","        kld_loss = (-0.5 * torch.sum((logv - torch.pow(mean, 2) - torch.exp(logv) + 1), 1)).mean()\n","\n","        loss += (kld_loss * 0.1)\n","\n","        # Backpropagation\n","        loss.backward(retain_graph=True)\n","        torch.nn.utils.clip_grad_norm_(self.encoder.parameters(), 0.5)\n","        torch.nn.utils.clip_grad_norm_(self.decoder.parameters(), 0.5)\n","        self.encoder_optimizer.step()\n","        self.decoder_optimizer.step()\n","\n","        result = loss.item() / (num_steps) \n","        return result\n","\n","\n","class Encoder(nn.Module):\n","\n","    def __init__(self, args, embedding):\n","        \n","        super(Encoder, self).__init__()\n","        self.args = args\n","        \n","        # Embedding layer\n","        self.embedding = embedding\n","        \n","        # RNN layer\n","        self.num_directions = 2 \n","        self.num_hidden_states = 1\n","        \n","        self.rnn = nn.GRU(input_size=100,\n","                          hidden_size=100,\n","                          num_layers=1,\n","                          batch_first=True,\n","                          bidirectional=True)\n","        \n","        # Initialize hidden state\n","        self.hidden = None\n","        \n","        self.linear_dims = [100 * 2 * 1 * 1]\n","                \n","        # Define last linear output layer\n","        self.hidden_to_mean = nn.Linear(self.linear_dims[-1], 1024)\n","        self.hidden_to_logv = nn.Linear(self.linear_dims[-1], 1024)\n","\n","        self._init_weights() \n","\n","    def init_hidden(self, batch_size):\n","        return torch.zeros(1 * 2, batch_size, 100).to(self.args.device)\n","        \n","    def _init_weights(self):\n","        for m in self.modules():\n","            if isinstance(m, nn.Linear):\n","                torch.nn.init.xavier_uniform_(m.weight)\n","                m.bias.data.fill_(0.01)\n","\n","\n","    def _sample(self, mean, logv):\n","        std = torch.exp(0.5 * logv)\n","        # torch.randn_like() creates a tensor with values samples from N(0,1) and std.shape\n","        eps = torch.randn_like(std)\n","        # Sampling from Z~N(μ, σ^2) = Sampling from μ + σX, X~N(0,1)\n","        z = mean + std * eps\n","        return z\n","\n","    def forward(self, inputs):\n","        # inputs.shape = (batch_size, seq_len)   \n","        batch_size, _ = inputs.shape\n","        \n","        # Push through embedding layer ==> X.shape = (batch_size, seq_len, embed_dim)\n","        X = self.embedding(inputs)\n","           \n","        _, self.hidden = self.rnn(X, self.hidden)\n","        \n","        X = self._flatten(self.hidden, batch_size)\n","        \n","        mean = self.hidden_to_mean(X)\n","        logv = self.hidden_to_logv(X)\n","        \n","        z = self._sample(mean, logv)\n","        \n","        return mean, logv, z\n","\n","    def _flatten(self, h, batch_size):\n","        # (num_layers*num_directions, batch_size, hidden_dim)  ==>\n","        # (batch_size, num_directions*num_layers, hidden_dim)  ==>\n","        # (batch_size, num_directions*num_layers*hidden_dim)\n","        return h.transpose(0,1).contiguous().view(batch_size, -1)\n","    \n","    \n","class Decoder(nn.Module):\n","    \n","    def __init__(self,args,embedding):\n","        super(Decoder, self).__init__()\n","\n","        self.criterion = nn.CrossEntropyLoss()\n","        self.args = args\n","        \n","        # Embedding layer\n","        self.embedding = embedding\n","        \n","        self.rnn = nn.GRU(100,\n","                       2*100,\n","                       num_layers=1,\n","                       batch_first=True)\n","        self.linear_dims = [100 * 2 * 1 * 1]\n","        self.z_to_hidden = nn.Linear(1024, self.linear_dims[0])\n","        \n","        self.out = nn.Linear(100 * 2, self.args.num_embeddings)\n","        \n","\n","    def forward(self, inputs, z, return_outputs=False):\n","        batch_size, num_steps = inputs.shape\n","        \n","        X = self.z_to_hidden(z)\n","        \n","        # Unflatten hidden state for GRU or LSTM\n","        hidden = self._unflatten(X, batch_size)\n","        \n","        # Restructure shape of hidden state to accommodate bidirectional encoder (decoder is unidirectional)\n","        hidden = self._init_hidden_state(hidden)\n","        \n","        # Create SOS token tensor as first input for decoder\n","        input = torch.LongTensor([[self.args.vectorizer.text_vocab.begin_seq_index]] * batch_size).to(self.args.device)\n","        \n","        # Initiliaze loss\n","        loss = 0\n","        outputs = torch.zeros((batch_size, num_steps), dtype=torch.long).to(self.args.device)\n","    \n","        for i in range(num_steps):\n","            output, hidden = self._step(input, hidden)\n","            topv, topi = output.topk(1)\n","            \n","            input = topi.detach()\n","            t = topi.detach().squeeze()\n","      \n","            outputs[:, i] = topi.detach().squeeze()\n","            \n","            loss += self.criterion(output, inputs[:, i])\n","            if input[0].item() == self.args.vectorizer.text_vocab.begin_seq_index:\n","                break\n","        \n","        # Return loss\n","        return loss, outputs\n","    \n","    \n","    def _unflatten(self, X, batch_size):\n","        # (batch_size, num_directions*num_layers*hidden_dim)    ==>\n","        # (batch_size, num_directions * num_layers, hidden_dim) ==>\n","        # (num_layers * num_directions, batch_size, hidden_dim) ==>\n","        return X.view(batch_size, 1 * 2, 100).transpose(0, 1).contiguous()\n","    \n","    def _init_hidden_state(self, encoder_hidden):\n","        return self._concat_directions(encoder_hidden)\n","\n","    def _concat_directions(self, hidden):\n","        hidden = torch.cat([hidden[0:hidden.size(0):2], hidden[1:hidden.size(0):2]], 2)\n","        return hidden\n","    \n","    def _step(self, input, hidden):\n","        # Get embedding of current input word:\n","        X = self.embedding(input)\n","        \n","        # Push input word through rnn layer with current hidden state\n","        output, hidden = self.rnn(X, hidden)\n","       \n","        # Push output through linear layer to get to vocab_size\n","        output = F.log_softmax(self.out(output.squeeze(dim=1)), dim=1)\n","       \n","        # return the output (batch_size, vocab_size) and new hidden state\n","        return output, hidden"],"execution_count":6,"outputs":[]},{"cell_type":"code","metadata":{"id":"WdcrDf7_iOh5","executionInfo":{"status":"ok","timestamp":1645593417614,"user_tz":-330,"elapsed":675,"user":{"displayName":"SARANSH GOEL","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"10247441952356680073"}}},"source":["import torch\n","import os\n","import numpy as np\n","\n","def make_train_state(args):\n","    return {'stop_early': False,\n","            'early_stopping_step': 0,\n","            'early_stopping_best_val': 1e8,\n","            'learning_rate': args.learning_rate,\n","            'epoch_index': 0,\n","            'train_loss': [],\n","            'train_acc': [],\n","            'val_loss': [],\n","            'val_acc': [],\n","            'test_loss': -1,\n","            'test_acc': -1,\n","            'model_filename': args.model_state_file}\n","\n","def update_train_state(args, model, train_state):\n","    \"\"\"Handle the training state updates.\n","    Components:\n","     - Early Stopping: Prevent overfitting.\n","     - Model Checkpoint: Model is saved if the model is better\n","    :param args: main arguments\n","    :param model: model to train\n","    :param train_state: a dictionary representing the training state values\n","    :returns:\n","        a new train_state\n","    \"\"\"\n","\n","    # Save one model at least\n","    if train_state['epoch_index'] == 0:\n","        torch.save(model.state_dict(), train_state['model_filename'])\n","        train_state['stop_early'] = False\n","\n","    # Save model if performance improved\n","    elif train_state['epoch_index'] >= 1:\n","        loss_tm1, loss_t = train_state['val_loss'][-2:]\n","\n","        # If loss worsened\n","        if loss_t >= train_state['early_stopping_best_val']:\n","            # Update step\n","            train_state['early_stopping_step'] += 1\n","        # Loss decreased\n","        else:\n","            # Save the best model\n","            if loss_t < train_state['early_stopping_best_val']:\n","                torch.save(model.state_dict(), train_state['model_filename'])\n","\n","            # Reset early stopping step\n","            train_state['early_stopping_step'] = 0\n","\n","        # Stop early ?\n","        train_state['stop_early'] = \\\n","            train_state['early_stopping_step'] >= args.early_stopping_criteria\n","\n","    return train_state\n","\n","def compute_accuracy(y_pred, y_target):\n","    _, y_pred_indices = y_pred.max(dim=1)\n","    n_correct = torch.eq(y_pred_indices, y_target).sum().item()\n","    return n_correct / len(y_pred_indices) * 100\n","\n","def set_seed_everywhere(seed, cuda):\n","    np.random.seed(seed)\n","    torch.manual_seed(seed)\n","    if cuda:\n","        torch.cuda.manual_seed_all(seed)\n","\n","def handle_dirs(dirpath):\n","    if not os.path.exists(dirpath):\n","        os.makedirs(dirpath)"],"execution_count":7,"outputs":[]},{"cell_type":"code","metadata":{"id":"WOcG-I5oidx7","executionInfo":{"status":"ok","timestamp":1645593417615,"user_tz":-330,"elapsed":33,"user":{"displayName":"SARANSH GOEL","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"10247441952356680073"}}},"source":["from argparse import Namespace\n","import torch\n","import os\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from tqdm import tqdm_notebook\n"],"execution_count":8,"outputs":[]},{"cell_type":"code","metadata":{"id":"ceMjhXQWbsym","executionInfo":{"status":"ok","timestamp":1645593417617,"user_tz":-330,"elapsed":32,"user":{"displayName":"SARANSH GOEL","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"10247441952356680073"}}},"source":["news_csv=\"/content/drive/MyDrive/FND using Unsupervised Learning /politifact/data/complete_processed_dataset_only_real_train.csv\""],"execution_count":9,"outputs":[]},{"cell_type":"code","metadata":{"id":"jLKL6h3BilmP","executionInfo":{"status":"ok","timestamp":1645593417618,"user_tz":-330,"elapsed":32,"user":{"displayName":"SARANSH GOEL","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"10247441952356680073"}}},"source":["args = Namespace(\n","    # Data and Path hyper parameters\n","    news_csv=\"/content/drive/MyDrive/FND using Unsupervised Learning /politifact/data/complete_processed_dataset_only_real_train.csv\",\n","    vectorizer_file=\"vectorizer.json\",\n","    model_state_file='/content/drive/MyDrive/FND using Unsupervised Learning /data/politifact/auto_model.pth',\n","    save_dir='/content/drive/MyDrive/FND using Unsupervised Learning /data/politifactmodels/classification',\n","    # Model hyper parameters\n","    glove_filepath='/content/drive/MyDrive/FND using Unsupervised Learning /data/politifact/glove.6B.100d.txt', \n","    use_glove=True,\n","    embedding_size=100, \n","    rnn_hidden_dim=100, \n","    num_channels=100, \n","    # Training hyper parameter\n","    seed=1337, \n","    learning_rate=0.001, \n","    dropout_p=0.1, \n","    batch_size=64, \n","    num_epochs=10, \n","    early_stopping_criteria=5, \n","    # Runtime option\n","    cuda=True, \n","    catch_keyboard_interrupt=True, \n","    reload_from_files=False,\n","    expand_filepaths_to_save_dir=True,\n","    # New params\n","    num_embeddings = None,\n","    num_classes = None,\n","    padding_idx = 0,\n","    linear_dims = [],\n","    z_dim = 1024,\n","    num_layers = 1,\n","    bidirectional_encoder = True,\n","    vectorizer = None,\n","    dropout = 0.5, \n",") "],"execution_count":10,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9gRj1gH2ituL","executionInfo":{"status":"ok","timestamp":1645593417619,"user_tz":-330,"elapsed":32,"user":{"displayName":"SARANSH GOEL","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"10247441952356680073"}},"outputId":"8040b052-96fd-4d2c-905f-82fbf1192362"},"source":["if args.expand_filepaths_to_save_dir:\n","    args.vectorizer_file = os.path.join(args.save_dir,\n","                                        args.vectorizer_file)\n","\n","    args.model_state_file = os.path.join(args.save_dir,\n","                                         args.model_state_file)\n","    \n","    print(\"Expanded filepaths: \")\n","    print(\"\\t{}\".format(args.vectorizer_file))\n","    print(\"\\t{}\".format(args.model_state_file))"],"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["Expanded filepaths: \n","\t/content/drive/MyDrive/FND using Unsupervised Learning /data/politifactmodels/classification/vectorizer.json\n","\t/content/drive/MyDrive/FND using Unsupervised Learning /data/politifact/auto_model.pth\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9PHtSPjGiyT7","executionInfo":{"status":"ok","timestamp":1645593417620,"user_tz":-330,"elapsed":23,"user":{"displayName":"SARANSH GOEL","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"10247441952356680073"}},"outputId":"6faa7f37-ab7f-4c22-ff01-3a624b88c79b"},"source":["# Check CUDA\n","if not torch.cuda.is_available():\n","    args.cuda = False\n","    \n","args.device = torch.device(\"cuda\" if args.cuda else \"cpu\")\n","print(\"Using CUDA: {}\".format(args.cuda))"],"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["Using CUDA: False\n"]}]},{"cell_type":"code","metadata":{"id":"1xnYdNGoi7FE","executionInfo":{"status":"ok","timestamp":1645593418846,"user_tz":-330,"elapsed":1245,"user":{"displayName":"SARANSH GOEL","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"10247441952356680073"}}},"source":["# Set seed for reproducibility\n","set_seed_everywhere(args.seed, args.cuda)\n","\n","# handle dirs\n","handle_dirs(args.save_dir)"],"execution_count":13,"outputs":[]},{"cell_type":"code","metadata":{"id":"OpxEkeLqi8PB","colab":{"base_uri":"https://localhost:8080/","height":413},"executionInfo":{"status":"error","timestamp":1645593418854,"user_tz":-330,"elapsed":26,"user":{"displayName":"SARANSH GOEL","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"10247441952356680073"}},"outputId":"6ad545bd-a3a2-4799-c830-f25086707e73"},"source":["if args.reload_from_files:\n","    # training from a checkpoint\n","    dataset = NewsDataset.load_dataset_and_load_vectorizer(args.news_csv, args.vectorizer_file)\n","else:\n","    # create dataset and vectorizer\n","    dataset = NewsDataset.load_dataset_and_make_vectorizer(args.news_csv)\n","    dataset.save_vectorizer(args.vectorizer_file)\n","vectorizer = dataset.get_vectorizer()\n","args.vectorizer = vectorizer\n","args.num_embeddings = len(vectorizer.text_vocab)\n","args.num_classes = len(vectorizer.type_vocab)"],"execution_count":14,"outputs":[{"output_type":"error","ename":"FileNotFoundError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-14-5dd53a048143>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;31m# create dataset and vectorizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNewsDataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_dataset_and_make_vectorizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnews_csv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_vectorizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvectorizer_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mvectorizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_vectorizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-4-0ce90292cb88>\u001b[0m in \u001b[0;36mload_dataset_and_make_vectorizer\u001b[0;34m(cls, news_csv)\u001b[0m\n\u001b[1;32m     44\u001b[0m             \u001b[0man\u001b[0m \u001b[0minstance\u001b[0m \u001b[0mof\u001b[0m \u001b[0mSurnameDataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m         \"\"\"\n\u001b[0;32m---> 46\u001b[0;31m         \u001b[0mnews_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnews_csv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m         \u001b[0mtrain_news_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnews_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnews_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnews_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNewsVectorizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_dataframe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_news_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m                 )\n\u001b[0;32m--> 311\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    584\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 586\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    587\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    480\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    481\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 482\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    483\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    484\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    809\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    810\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 811\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    812\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    813\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1038\u001b[0m             )\n\u001b[1;32m   1039\u001b[0m         \u001b[0;31m# error: Too many arguments for \"ParserBase\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1040\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mmapping\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1041\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1042\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_failover_to_python\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/c_parser_wrapper.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0;31m# open handles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_open_handles\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/base_parser.py\u001b[0m in \u001b[0;36m_open_handles\u001b[0;34m(self, src, kwds)\u001b[0m\n\u001b[1;32m    227\u001b[0m             \u001b[0mmemory_map\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"memory_map\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m             \u001b[0mstorage_options\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"storage_options\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 229\u001b[0;31m             \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"encoding_errors\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"strict\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    230\u001b[0m         )\n\u001b[1;32m    231\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    705\u001b[0m                 \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    706\u001b[0m                 \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 707\u001b[0;31m                 \u001b[0mnewline\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    708\u001b[0m             )\n\u001b[1;32m    709\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drive/MyDrive/FND using Unsupervised Learning /politifact/data/complete_processed_dataset_only_real_train.csv'"]}]},{"cell_type":"code","metadata":{"id":"0f8ST7V29yfm","executionInfo":{"status":"aborted","timestamp":1645593418846,"user_tz":-330,"elapsed":15,"user":{"displayName":"SARANSH GOEL","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"10247441952356680073"}}},"source":["# Use GloVe or randomly initialized embeddings\n","if args.use_glove:\n","    words = vectorizer.text_vocab._token_to_idx.keys()\n","    embeddings = make_embedding_matrix(glove_filepath=args.glove_filepath, \n","                                       words=words)\n","    print(\"Using pre-trained embeddings\")\n","else:\n","    print(\"Not using pre-trained embeddings\")\n","    embeddings = None"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"HZpAb55r-NdS","executionInfo":{"status":"aborted","timestamp":1645593418847,"user_tz":-330,"elapsed":16,"user":{"displayName":"SARANSH GOEL","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"10247441952356680073"}}},"source":["model =  TextRnnVAE(args,embeddings)\n","\n","#print(model)\n","\n","model = model.to(args.device)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"CpPk0EXEvUyp","executionInfo":{"status":"aborted","timestamp":1645593418848,"user_tz":-330,"elapsed":17,"user":{"displayName":"SARANSH GOEL","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"10247441952356680073"}}},"source":["losses = []\n","model.train()\n","\n","for epoch in range(args.num_epochs):\n","    dataset.set_split('train')\n","    batch_generator = generate_batches(dataset, \n","                                      batch_size=args.batch_size, \n","                                      device=args.device)\n","    epoch_loss = model.train_epoch(epoch, batch_generator)\n","    print(epoch_loss)\n","    losses.append(epoch_loss)"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["epochs = [(i+1) for i in range(args.num_epochs)]\n","epochs"],"metadata":{"id":"v0RUciYTb3Ke","executionInfo":{"status":"aborted","timestamp":1645593418849,"user_tz":-330,"elapsed":17,"user":{"displayName":"SARANSH GOEL","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"10247441952356680073"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import matplotlib.pyplot as plt\n","\n","plt.plot(epochs, losses)\n","plt.xlabel(\"No. of epochs\")  # add X-axis label\n","plt.ylabel(\"Loss\")  # add Y-axis label\n","plt.show()"],"metadata":{"id":"NUXVcigAdRGI","executionInfo":{"status":"aborted","timestamp":1645593418849,"user_tz":-330,"elapsed":17,"user":{"displayName":"SARANSH GOEL","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"10247441952356680073"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"zRkUMG8qLVQ4","executionInfo":{"status":"aborted","timestamp":1645593418850,"user_tz":-330,"elapsed":18,"user":{"displayName":"SARANSH GOEL","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"10247441952356680073"}}},"source":["torch.save(model,\"/content/drive/MyDrive/FND using Unsupervised Learning /politifact/models/trained_model.pth\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def calc_reconstruction_error(txt_vector):\n","    model.encoder.eval()\n","    model.decoder.eval()\n","    model.encoder.hidden = model.encoder.init_hidden(1)\n","    mean, logv, z = model.encoder(txt_vector)\n","    loss,o = model.decoder(txt_vector, z)\n","    return loss"],"metadata":{"id":"56WObztfazSe","executionInfo":{"status":"aborted","timestamp":1645593418851,"user_tz":-330,"elapsed":19,"user":{"displayName":"SARANSH GOEL","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"10247441952356680073"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1A5c2PFD8Gt4","executionInfo":{"status":"aborted","timestamp":1645593418851,"user_tz":-330,"elapsed":19,"user":{"displayName":"SARANSH GOEL","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"10247441952356680073"}}},"source":["dataset.set_split(\"test\")\n","\n","fake_losses = []\n","real_losses = []\n","fake_indices = []\n","real_indices = []\n","\n","for ind in dataset.test_df.index:\n","    text = dataset.test_df['text'][ind]\n","    label = dataset.test_df['type'][ind]\n","    \n","    text = vectorizer.vectorize(text,3062)\n","    text = torch.tensor(text)\n","    text = torch.unsqueeze(text,dim=0)\n","    text = text.to(device=args.device)\n","\n","    loss = calc_reconstruction_error(text)\n","    loss = loss.item()\n","    if(label == 0):\n","        fake_losses.append(loss)\n","        fake_indices.append(ind)\n","    else:\n","        real_losses.append(loss)\n","        real_indices.append(ind)      \n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xZtOaUa2iivD","executionInfo":{"status":"aborted","timestamp":1645593418852,"user_tz":-330,"elapsed":19,"user":{"displayName":"SARANSH GOEL","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"10247441952356680073"}}},"source":["plt.scatter(fake_indices, fake_losses, c =\"blue\")\n","plt.scatter(real_indices, real_losses, c =\"red\")\n","\n","plt.xlabel(\"Sample index\")\n","plt.ylabel(\"Reconstruction error\")\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"qH54t9UeNa9M","executionInfo":{"status":"aborted","timestamp":1645593418852,"user_tz":-330,"elapsed":19,"user":{"displayName":"SARANSH GOEL","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"10247441952356680073"}}},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"wFCNiw5MaY9o","executionInfo":{"status":"aborted","timestamp":1645593418853,"user_tz":-330,"elapsed":19,"user":{"displayName":"SARANSH GOEL","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"10247441952356680073"}}},"source":[""],"execution_count":null,"outputs":[]}]}