{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"preprocessing+summarizer.ipynb","provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["!pip install transformers==4.10.0\n","!pip install bert-extractive-summarizer"],"metadata":{"id":"rskjR473nCgz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"Gjm9ltuVkcUI","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1645518580117,"user_tz":-330,"elapsed":22161,"user":{"displayName":"SARANSH GOEL","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"10247441952356680073"}},"outputId":"80ed7e50-9c1b-4f41-96f5-cafb60e1b27a"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["import json \n","from transformers import T5Tokenizer, T5ForConditionalGeneration, T5Config"],"metadata":{"id":"dEzXNmjF0AWG","executionInfo":{"status":"ok","timestamp":1645518595939,"user_tz":-330,"elapsed":10031,"user":{"displayName":"SARANSH GOEL","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"10247441952356680073"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["from transformers import AutoTokenizer, AutoModelWithLMHead\n","import torch\n","\n","tokenizer = AutoTokenizer.from_pretrained('t5-base')\n","model = AutoModelWithLMHead.from_pretrained('t5-base', return_dict=True)\n","device = torch.device(\"cpu\")\n","model = model.to(device)"],"metadata":{"id":"KmK80Ine0HSV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","\n","import re\n","import nltk\n","from nltk.tokenize import RegexpTokenizer\n","from nltk.corpus import stopwords\n","nltk.download('stopwords')\n","\n","from textblob import Word\n","\n","import random\n","seed_val = 42\n","random.seed(seed_val)\n","np.random.seed(seed_val)\n","\n","import torch\n","# from summarizer import Summarizer,TransformerSummarizer\n","import warnings\n","warnings.filterwarnings(\"ignore\")\n","\n","\n","# Read csv and select required cols\n","df = pd.read_csv(\"/content/drive/MyDrive/FND using Unsupervised Learning /data/gossipcop/news_article_dataset.csv\")\n","cols_to_select = [\"id\", \"text\",\"type\"]\n","df = df[cols_to_select]\n","\n","def preprocess(txt):\n","    try:\n","        txt = txt.decode('utf-8').lower()\n","    except:\n","        try:\n","            txt = txt.encode('utf-8').decode('utf-8').lower()\n","        except:\n","            print(\"TXT: \"+str(txt))\n","            return txt\n","    txt = re.sub(r'\\s+',' ',txt)\n","    txt = re.sub(r\"\\\\n\", \" \",txt)\n","    txt = re.sub(r'@[\\w\\-]+','',txt)\n","    txt = re.sub(r'https?:\\/\\/\\S+','',txt)\n","    txt = re.sub(r\"www\\.[a-z]?\\.?(com)+|[a-z]+\\.(com)\",'',txt)\n","    txt = re.sub(r'{link}', '', txt)\n","    txt = re.sub(r\"\\[video\\]\", '', txt)\n","    txt = re.sub(r'&[a-z]+;', '', txt)\n","    txt = re.sub(r\"[^a-z\\s\\(\\-:\\)\\\\\\/\\];='#]\", '', txt)\n","    txt = re.sub(r\"[^\\w\\d'\\s]+\",'',txt)\n","    return txt\n","\n","\n","stop_words = stopwords.words('english')\n","def get_tokens(tweet):\n","    tweet = preprocess(tweet)\n","    tokenizer = RegexpTokenizer(r'\\w+')\n","    tokens = tokenizer.tokenize(tweet)\n","      \n","    # Think whether to include this or not?\n","    #words = [Word(t).lemmatize() for t in tokens]\n","    #words = [w for w in words if not w in stop_words]\n","    \n","    words = ' '.join(tokens)\n","    return words\n","\n","\"\"\"\n","def summarizer(body):\n","    GPT2_model = TransformerSummarizer(transformer_type=\"GPT2\",transformer_model_key=\"gpt2-medium\")\n","    summarized_tokens = GPT2_model(body)\n","    summary = ''.join(summarized_tokens)\n","    return summary\n","\"\"\"\n","\n","\n","\n","def summarizer(body):\n","    inputs = tokenizer.encode(\"summarize: \" + body,return_tensors='pt',max_length=512,truncation=True).to(device)\n","    summary_ids = model.generate(inputs, max_length=150, min_length=80, length_penalty=5., num_beams=2).to(device)\n","    summary = tokenizer.decode(summary_ids[0],skip_special_tokens=True)\n","    # print(summary)\n","    return summary\n","\n","\n","drop_rows = []\n","for i in range(df.shape[0]):\n","    try:    \n","        drop_rows.append(not np.isnan(df['text'].loc[i]))\n","    except:\n","        drop_rows.append(True)\n","\n","df = df[drop_rows]\n","\n","#df['text'] = df['text'].apply(lambda x: summarizer(x))\n","df['text'] = df['text'].apply(lambda x: get_tokens(x))\n","\n","df.reset_index(drop=True)\n","\n","max_len = 0\n","for i in range(df.shape[0]):\n","    tweet = df['text'].iloc[i]\n","    tokenizer = RegexpTokenizer(r'\\w+')\n","    tokens = tokenizer.tokenize(tweet)\n","    max_len = max(max_len,len(tokens))\n","\n","print(\"MAX LEN: \"+str(max_len))\n","\n","# /content/drive/MyDrive/FND using Unsupervised Learning /data/gossipcop/news_article_dataset.csv\n","df.to_csv(\"/content/drive/MyDrive/FND using Unsupervised Learning /data/gossipcop/processed_news_article_dataset.csv\")"],"metadata":{"id":"Nx3_WkKBkz5z","colab":{"base_uri":"https://localhost:8080/"},"outputId":"cd7bbe1e-fffd-46d6-9353-ca0e377cf8f6"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n"]}]},{"cell_type":"code","source":[""],"metadata":{"id":"q4dxgvtKm5b1"},"execution_count":null,"outputs":[]}]}